{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install whoosh\n",
    "!pip install pytrec_eval\n",
    "!pip install wget\n",
    "!pip install --upgrade gensim\n",
    "!pip install --user -U nltk\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download(\"https://github.com/MIE451-1513-2019/course-datasets/raw/master/government.zip\", \"government.zip\")\n",
    "wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \"GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# Put all your imports here\n",
    "from whoosh import index, writing\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import QueryParser\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import subprocess\n",
    "import pytrec_eval\n",
    "import wget\n",
    "import numpy as np\n",
    "from whoosh import fields\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from whoosh.analysis.tokenizers import Tokenizer\n",
    "from whoosh.analysis.acore import Token\n",
    "from whoosh.scoring import WeightingModel, BaseScorer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from whoosh.fields import FieldType\n",
    "from whoosh.analysis.analyzers import Analyzer\n",
    "from whoosh import analysis\n",
    "from whoosh import query\n",
    "from whoosh import formats\n",
    "from whoosh.formats import Format, Existence\n",
    "from whoosh.system import emptybytes\n",
    "from whoosh.analysis.filters import STOP_WORDS\n",
    "from whoosh.searching import Results\n",
    "from whoosh.scoring import BM25F\n",
    "from whoosh.lang import *\n",
    "\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(value, analyzer, kwargs):\n",
    "    if isinstance(value, (tuple, list)):\n",
    "        gen = entoken(value, **kwargs)\n",
    "    else:\n",
    "        gen = analyzer(value, **kwargs)\n",
    "    return unstopped(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customTokenizer(Tokenizer):\n",
    "  def __init__(self, remove_nonalpha=False, keep_special=False, lemmatize=False, stop_words_list=STOP_WORDS):\n",
    "    self.lemmatize = lemmatize\n",
    "    self.remove_nonalpha = remove_nonalpha\n",
    "    self.keep_special = keep_special\n",
    "    self.stop_words_list = stop_words_list\n",
    "\n",
    "  def __call__(self, value, positions=False, stem=None, lemmatize=None, chars=False, keeporiginal=False,\n",
    "                 removestops=False, start_pos=0, start_char=0, tokenize=True,\n",
    "                 mode='', **kwargs):\n",
    "    \n",
    "    t = Token(positions, chars, removestops=removestops, mode=mode,\n",
    "              **kwargs)\n",
    "    \n",
    "    # tokenizing words from text corpusn\n",
    "    tokenizer = RegexpTokenizer(r'[\\d.,]+|[A-Z][.A-Z]+\\b\\.*|\\w+|\\S') # Parsing words connected by '-' into separate words, and keeping special words connected by '.', like 'U.S.'\n",
    "    tokenized_words = tokenizer.tokenize(value)\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    # If True, keep special words like 'U.S.' unchanged and make everything else lowercase\n",
    "    # else, make everything lowercase\n",
    "    if self.keep_special:\n",
    "      for word in tokenized_words:\n",
    "        if word not in string.punctuation:\n",
    "          if any(char == '.' for char in word):\n",
    "            filtered_words.append(word)\n",
    "          else:\n",
    "            filtered_words.append(word.lower())\n",
    "    else:\n",
    "      filtered_words = [w.lower() for w in tokenized_words if w not in string.punctuation]\n",
    "\n",
    "    # If True, remove non-alphabetic characters (numbers and punctuations) \n",
    "    if self.remove_nonalpha:\n",
    "      filtered_words2 = []\n",
    "      for word in filtered_words:\n",
    "        if any(char.isnumeric() or char=='_' for char in word) or word in string.punctuation:\n",
    "          continue\n",
    "        else:\n",
    "          filtered_words2.append(word)\n",
    "      filtered_words = filtered_words2\n",
    "    \n",
    "    # If True, lemmatize words\n",
    "    if self.lemmatize:\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      filtered_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "\n",
    "    # remove stop words\n",
    "    filtered_words = [w for w in filtered_words if w not in self.stop_words_list]\n",
    "\n",
    "    for w in filtered_words:\n",
    "      t.text = w\n",
    "      t.boost = 1.0\n",
    "      t.pos = start_pos\n",
    "      start_pos += 1\n",
    "      yield t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecFilter(Filter):\n",
    "  def __call__(self, tokens):\n",
    "    # Converting from words to word embeddings with Word2Vec\n",
    "    for t in tokens:\n",
    "      try:\n",
    "        embedding = word2vec[t.text]\n",
    "      except Exception as e:\n",
    "        embedding  = np.array([])\n",
    "\n",
    "      t.embedding = embedding\n",
    "\n",
    "      yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing document centroids\n",
    "def compute_centroid(word_embeddings, schema):    \n",
    "    centroid = np.zeros((300,))\n",
    "    count = 0\n",
    "    for w in word_embeddings:\n",
    "      if len(w)!=0:\n",
    "        centroid += w/np.linalg.norm(w)\n",
    "        count += 1\n",
    "    centroid = centroid/count\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Searcher object for ranking documents based on cosine distance between \n",
    "# their centroids and queries\n",
    "class Searcher(object):\n",
    "  def __init__(self, index):\n",
    "    self.index = index\n",
    "    self.ixreader = index.reader()\n",
    "    self.is_closed = False\n",
    "    self._doccount = self.ixreader.doc_count_all()\n",
    "    # Cache for PostingCategorizer objects (supports fields without columns)\n",
    "    self._field_caches = {}\n",
    "    self.centroids = self.compute_document_centroids()\n",
    "    # Copy attributes/methods from wrapped reader\n",
    "    for name in (\"stored_fields\", \"all_stored_fields\", \"has_vector\",\n",
    "                  \"vector\", \"vector_as\", \"lexicon\", \"field_terms\",\n",
    "                  \"frequency\", \"doc_frequency\", \"term_info\",\n",
    "                  \"doc_field_length\", \"corrector\", \"iter_docs\"):\n",
    "        setattr(self, name, getattr(self.ixreader, name))\n",
    "\n",
    "  def compute_document_centroids(self):\n",
    "    centroids = {}\n",
    "    for i in self.ixreader.iter_docs():\n",
    "      text = i[1]['file_content']\n",
    "      word_embeddings = self.index.schema['file_content'].process_text(text)\n",
    "      centroid = compute_centroid(word_embeddings, self.index.schema)\n",
    "      centroids[i[0]] = centroid\n",
    "\n",
    "    return centroids\n",
    "\n",
    "  def search(self, q, top_n = 20, **kwargs):\n",
    "    scores = []\n",
    "    vecs = []\n",
    "    if type(q) == query.terms.Term:\n",
    "      q = [q]\n",
    "\n",
    "    for i in q:\n",
    "      try:\n",
    "        vec = np.fromstring(i.text, dtype=float, sep=' ')\n",
    "        vecs.append(vec)\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "    for docnum in self.centroids:\n",
    "      score = 0\n",
    "      # compute similarity between each query term and document centroid, \n",
    "      # and takes the average\n",
    "      for q in vecs:\n",
    "        score += cosine_sim(q, self.centroids[docnum])\n",
    "      score = score/len(vecs)\n",
    "      scores.append((score, docnum))\n",
    "    \n",
    "    sorted_scores = sorted(scores, key = lambda x: x[0], reverse=True)\n",
    "\n",
    "    results = Results(self, q, sorted_scores[:top_n])\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
